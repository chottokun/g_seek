# General Application Configuration / 全般設定
# ---------------------------------
# レポートのデフォルト出力ファイル名。
# OUTPUT_FILENAME="my_research_report.md"

# ログ出力レベル。DEBUG, INFO, WARNING, ERROR から選択。
# LOG_LEVEL="INFO"

# デフォルトの言語設定（レポート作成や要約で使用）。
# DEFAULT_LANGUAGE="Japanese"

# レポートが保存されるディレクトリ。
# REPORT_DIR="temp_reports"

# 一時ファイルのクリーンアップ対象となる保持期間（秒）。
# CLEANUP_AGE_SECONDS=86400 # 24時間


# UI Port Configuration / UIポート設定 (任意)
# ---------------------------------
# Chainlit UIが使用するポート番号。デフォルトは8000。
# CHAINLIT_PORT=8000

# Streamlit UIが使用するポート番号。デフォルトは8501。
# STREAMLIT_SERVER_PORT=8501


# LLM Configuration / LLM設定
# -----------------
# LLMプロバイダーの選択。
# 指定可能な値: "openai", "azure_openai", "ollama", "placeholder_llm"
LLM_PROVIDER="openai"

# 選択したプロバイダーのモデル名。
# - OpenAIの場合: "gpt-3.5-turbo", "gpt-4", "gpt-4o" など
# - Azure OpenAIの場合: デプロイ名（AZURE_OPENAI_DEPLOYMENT_NAMEで上書きされます）
# - Ollamaの場合: "llama3", "mistral", "phi3" など
LLM_MODEL="gpt-4o-mini"

# LLMが1回のリクエストで生成する最大トークン数。
LLM_MAX_TOKENS=2048

# LLM出力のランダム性を制御します。値が低いほど決定的になります (0.0 から 2.0)。
LLM_TEMPERATURE=0.7


# OpenAI Specific Configuration / OpenAI個別設定
# -----------------------------
# OpenAI APIキー。
# OPENAI_API_KEY="sk-..."

# APIのベースURL。LiteLLMやローカルLLMなどのプロキシを使用する場合に指定。
# OPENAI_BASE_URL="http://localhost:8000/v1"
OPENAI_BASE_URL=


# Azure OpenAI Specific Configuration / Azure OpenAI個別設定
# ----------------------------------
# AZURE_OPENAI_API_KEY=""
# AZURE_OPENAI_ENDPOINT="" # 例: https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION="" # 例: "2024-02-15-preview"
# AZURE_OPENAI_DEPLOYMENT_NAME="" # Azureでのモデルデプロイ名


# Ollama Specific Configuration / Ollama個別設定
# -----------------------------
# OllamaサービスのベースURL。
# デフォルトは http://localhost:11434 です。
# OLLAMA_BASE_URL="http://localhost:11434"


# Search Configuration / 検索設定
# --------------------
# 使用する検索エンジン。
# 指定可能な値: "duckduckgo" (デフォルト), "tavily", "searxng"
SEARCH_API="duckduckgo"

# SEARCH_API="tavily" の場合に必要。
# TAVILY_API_KEY=""

# SEARCH_API="searxng" の場合に必要。
# デフォルトは http://localhost:8080 です。
# SEARXNG_BASE_URL="http://localhost:8080"

# SearxNG固有の設定
# SEARXNG_LANGUAGE="ja-JP"
# SEARXNG_SAFESEARCH=1 # 0: Off, 1: Moderate, 2: Strict
# SEARXNG_CATEGORIES="general"


# Research Loop Configuration / リサーチループ設定
# ---------------------------
# 最大リサーチループ回数（クエリ生成 -> 検索 -> 要約 -> 内省のサイクル）。
MAX_RESEARCH_LOOPS="3"

# 1つのクエリに対して取得する最大検索結果数。
MAX_SEARCH_RESULTS_PER_QUERY="5"

# 検索クエリに含まれる最大単語数。
# MAX_QUERY_WORDS=15

# 生成されるリサーチプランの最小/最大セクション数。
# RESEARCH_PLAN_MIN_SECTIONS=3
# RESEARCH_PLAN_MAX_SECTIONS=10


# Content Processing Configuration / コンテンツ処理設定
# --------------------------------
# 検索結果のスニペットのみを使用するかどうか。
# "true" の場合、高速ですが詳細さは低下します。"false" の場合、URLから全文取得を試みます。
USE_SNIPPETS_ONLY_MODE="False"

# 1つのソースから処理する最大文字数（0は無制限）。
# チャンク分割の前にこの長さで切り捨てられます。
MAX_TEXT_LENGTH_PER_SOURCE_CHARS=15000

# コンテンツ取得時のタイムアウト秒数。
# RETRIEVAL_TIMEOUT=15

# HTTPリクエスト時に使用するUser-Agentヘッダー。
# USER_AGENT="DeepResearchBot/1.0"

# 要約処理時のチャンクサイズ（文字数）。
# チャンクが大きいほど文脈が維持されますが、LLMの制限に注意が必要です。
SUMMARIZATION_CHUNK_SIZE_CHARS=8000

# 連続するチャンク間のオーバーラップ（重なり）文字数。
SUMMARIZATION_CHUNK_OVERLAP_CHARS=500

# PDFファイルをダウンロードしてテキストを抽出するかどうか。
# 実行には `pypdf` ライブラリが必要です。
PROCESS_PDF_FILES="True"


# Optimization Configuration / 最適化設定
# --------------------------
# 並列実行するチャンク要約タスクの最大数。
MAX_CONCURRENT_CHUNKS=4

# LLM APIへの1分間あたりのリクエスト制限（RPM）。
# APIプロバイダーの制限に合わせて調整してください。
LLM_RATE_LIMIT_RPM=60


# Interactive Mode / インタラクティブモード
# ----------------------------------------
# 各ステップの実行前にユーザーの承認を必要とするかどうか。
# CLI実行時に有効。UIではスイッチで制御される場合があります。
# INTERACTIVE_MODE="False"
