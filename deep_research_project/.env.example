# General Application Configuration
# ---------------------------------
# OUTPUT_FILENAME="my_research_report.md"
# LOG_LEVEL="INFO" # Options: DEBUG, INFO, WARNING, ERROR

# LLM Configuration
# -----------------
# LLM_PROVIDER: Specifies the LLM provider.
# Options: "openai", "azure_openai", "ollama", "placeholder_llm" (default if not set)
LLM_PROVIDER="openai"

# LLM_MODEL: The model name or identifier for the selected provider.
# - For OpenAI: e.g., "gpt-3.5-turbo", "gpt-4"
# - For Azure OpenAI: This should be your Azure Deployment Name (see AZURE_OPENAI_DEPLOYMENT_NAME below, this value will be overridden by it if provider is azure_openai)
# - For Ollama: e.g., "llama2", "mistral"
# - For placeholder_llm: "default_model_name" (or any, it's simulated)
LLM_MODEL="gpt-3.5-turbo"

# LLM_MAX_TOKENS: Maximum number of tokens the LLM should generate in a single response.
LLM_MAX_TOKENS=1024

# LLM_TEMPERATURE: Controls the randomness of the LLM's output. Lower is more deterministic. (0.0 to 2.0)
LLM_TEMPERATURE=0.7

# OpenAI Specific Configuration (used if LLM_PROVIDER="openai")
# -----------------------------
# OPENAI_API_KEY: Your OpenAI API key. Often set directly in the environment for security.
# OPENAI_API_KEY="sk-..."

# OPENAI_API_BASE_URL: Optional. Use for OpenAI-compatible proxies like LiteLLM or local LLMs.
# If set, it will override the default OpenAI API URL.
# Example: OPENAI_API_BASE_URL="http://localhost:8000/v1"
OPENAI_API_BASE_URL=

# Azure OpenAI Specific Configuration (used if LLM_PROVIDER="azure_openai")
# ----------------------------------
# Uncomment and provide your Azure OpenAI details if using this provider.
# AZURE_OPENAI_API_KEY=""
# AZURE_OPENAI_ENDPOINT="" # e.g., https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION="" # e.g., "2023-07-01-preview", "2023-12-01-preview"
# AZURE_OPENAI_DEPLOYMENT_NAME="" # Your model deployment name in Azure. This will be used as the effective model.

# Ollama Specific Configuration (used if LLM_PROVIDER="ollama")
# -----------------------------
# OLLAMA_BASE_URL: Optional. Base URL for the Ollama service.
# Defaults to http://localhost:11434 if Ollama is running locally and this is not set.
# OLLAMA_BASE_URL="http://localhost:11434"


# Search Configuration
# --------------------
# SEARCH_API: The search engine to use.
# Options: "duckduckgo" (default), "tavily"
SEARCH_API="duckduckgo"

# TAVILY_API_KEY: Required if SEARCH_API="tavily".
# TAVILY_API_KEY=""


# Research Loop Configuration
# ---------------------------
# MAX_RESEARCH_LOOPS: Maximum number of research cycles (query -> search -> summarize -> reflect) to perform.
MAX_RESEARCH_LOOPS="3"

# MAX_SEARCH_RESULTS_PER_QUERY: Maximum number of search results to fetch for each query.
MAX_SEARCH_RESULTS_PER_QUERY="3"


# Content Processing Configuration
# --------------------------------
# USE_SNIPPETS_ONLY_MODE: If "true", only search result snippets are used for summarization (faster, less detail).
# If "false", attempts to download and process full content from URLs.
USE_SNIPPETS_ONLY_MODE="False"

# MAX_TEXT_LENGTH_PER_SOURCE_CHARS: Maximum characters to process per web source (0 for unlimited).
# Truncation occurs before chunking.
MAX_TEXT_LENGTH_PER_SOURCE_CHARS=0

# PROCESS_PDF_FILES: If "true", attempts to download and extract text from PDF links.
# If "false", PDF links are skipped. Requires `pypdf` to be installed.
PROCESS_PDF_FILES="True"


# UI / Interactive Mode (These are typically not set via .env for Streamlit, but shown for completeness)
# ----------------------------------------------------------------------------------------------------
# INTERACTIVE_MODE: Controls if the research loop requires manual approval for steps.
# "true" for interactive (manual approval), "false" for automated.
# For CLI runs, this can be set. For Streamlit, it's controlled by a UI toggle.
# INTERACTIVE_MODE="True"

# Note: For sensitive keys like OPENAI_API_KEY or AZURE_OPENAI_API_KEY,
# it's often recommended to set them directly as environment variables in your system
# or use a secrets management solution, rather than hardcoding them in a .env file,
# especially if this file might be shared or version-controlled (even if it's .env.example).
# However, for local development, .env files (loaded by python-dotenv) are common.
# Ensure your actual .env file (if used) is in .gitignore.
# The .env.example file serves as a template and should not contain real secrets.
