diff --git a/deep_research_project/core/execution.py b/deep_research_project/core/execution.py
new file mode 100644
index 0000000..7b29aec
--- /dev/null
+++ b/deep_research_project/core/execution.py
@@ -0,0 +1,168 @@
+from typing import List, Optional, Callable, Dict
+import logging
+import asyncio
+from deep_research_project.config.config import Configuration
+from deep_research_project.core.state import ResearchState, SearchResult, Source, KnowledgeGraphModel
+from deep_research_project.tools.llm_client import LLMClient
+from deep_research_project.tools.search_client import SearchClient
+from deep_research_project.tools.content_retriever import ContentRetriever
+from deep_research_project.core.utils import split_text_into_chunks
+
+logger = logging.getLogger(__name__)
+
+class ExecutionManager:
+    def __init__(self, llm_client: LLMClient, search_client: SearchClient, content_retriever: ContentRetriever, config: Configuration):
+        self.llm_client = llm_client
+        self.search_client = search_client
+        self.content_retriever = content_retriever
+        self.config = config
+
+    async def generate_initial_query(self, state: ResearchState, section_title: str, section_desc: str) -> Optional[str]:
+        topic = section_title
+        desc = section_desc
+        logger.info(f"Generating initial query for: {topic}")
+        try:
+            max_words = getattr(self.config, "MAX_QUERY_WORDS", 12)
+            if state.language == "Japanese":
+                prompt = (
+                    f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€ç°¡æ½”ãªWebæ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆæœ€å¤§{max_words}å˜èªï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
+                    f"ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ: {state.research_topic}\n"
+                    f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {topic}\n"
+                    f"èª¬æ˜: {desc}\n\n"
+                    f"ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è‹±èªã®ã‚½ãƒ¼ã‚¹ã‚‚å–å¾—ã§ãã‚‹ã‚ˆã†ã€é©åˆ‡ã§ã‚ã‚Œã°è‹±èªã®ã‚¯ã‚¨ãƒªã‚‚æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
+                )
+            else:
+                prompt = (
+                    f"Generate a concise web search query (max {max_words} words) for the following research task.\n"
+                    f"Main Topic: {state.research_topic}\n"
+                    f"Section: {topic}\n"
+                    f"Description: {desc}\n\n"
+                    f"Output only the query."
+                )
+            query = await self.llm_client.generate_text(prompt=prompt)
+            return query
+        except Exception as e:
+            logger.error(f"Error generating query: {e}")
+            return None
+
+    async def web_search(self, query: str, progress_callback: Optional[Callable[[str], None]] = None) -> List[SearchResult]:
+        if not query: return []
+        logger.info(f"Performing web search for: {query}")
+        if progress_callback: await progress_callback(f"Searching web for: '{query}'...")
+        try:
+            results = await self.search_client.search(query, num_results=self.config.MAX_SEARCH_RESULTS_PER_QUERY)
+            if progress_callback:
+                if results:
+                    results_str = "\n".join([f"- [{r['title']}]({r['link']})" for r in results])
+                    await progress_callback(f"Found {len(results)} potential sources:\n{results_str}")
+                else:
+                    await progress_callback("No search results found.")
+            return results
+        except Exception as e:
+            logger.error(f"Error during search: {e}")
+            if progress_callback: await progress_callback(f"Search failed: {e}")
+            return []
+
+    async def summarize_sources(self, state: ResearchState, selected_results: List[SearchResult], progress_callback: Optional[Callable[[str], None]] = None):
+        if not selected_results:
+            state.new_information = "No sources selected."
+            state.pending_source_selection = False
+            return
+
+        if progress_callback:
+             sources_titles = ", ".join([r['title'] for r in selected_results])
+             await progress_callback(f"Summarizing {len(selected_results)} sources: {sources_titles}...")
+
+        all_chunk_summaries = []
+        all_chunks_info = [] # Store all chunks to be processed in parallel
+        if state.fetched_content is None: state.fetched_content = {}
+
+        for result in selected_results:
+            url = result['link']
+            if url not in state.fetched_content:
+                if self.config.USE_SNIPPETS_ONLY_MODE:
+                    content = result.get('snippet', '')
+                else:
+                    content = await self.content_retriever.retrieve_and_extract(url)
+                    if not content: content = result.get('snippet', '')
+                state.fetched_content[url] = content
+
+            content = state.fetched_content[url]
+            chunks = split_text_into_chunks(content, self.config.SUMMARIZATION_CHUNK_SIZE_CHARS, self.config.SUMMARIZATION_CHUNK_OVERLAP_CHARS)
+            all_chunks_info.extend([(chunk, url) for chunk in chunks])
+
+        # Limit concurrency using Semaphore
+        semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_CHUNKS)
+
+        async def summarize_chunk(chunk_info):
+            chunk, url = chunk_info
+            if state.is_interrupted: return None
+            async with semaphore:
+                if progress_callback: await progress_callback(f"Summarizing chunk from {url}...")
+                if state.language == "Japanese":
+                    prompt = f"ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ãƒª: '{state.current_query}' ã®ãŸã‚ã«ã€ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã‚»ã‚°ãƒ¡ãƒ³ãƒˆ:\n{chunk}"
+                else:
+                    prompt = f"Summarize this segment for the research query: '{state.current_query}'.\n\nSegment:\n{chunk}"
+                return await self.llm_client.generate_text(prompt=prompt)
+
+        # Execute parallel summarization
+        if all_chunks_info:
+            if progress_callback: await progress_callback(f"Starting parallel summarization for {len(all_chunks_info)} chunks...")
+            summaries = await asyncio.gather(*[summarize_chunk(info) for info in all_chunks_info])
+            all_chunk_summaries.extend([s for s in summaries if s])
+
+        if state.is_interrupted:
+             return
+
+        if not all_chunk_summaries:
+            state.new_information = "Could not summarize any content."
+            if progress_callback: await progress_callback("No content could be summarized.")
+        else:
+            if progress_callback: await progress_callback("Synthesizing final summary for the query...")
+            combined = "\n\n---\n\n".join(all_chunk_summaries)
+            if state.language == "Japanese":
+                prompt = f"ã“ã‚Œã‚‰ã®è¦ç´„ã‚’ã€ã‚¯ã‚¨ãƒª: '{state.current_query}' ã«é–¢ã™ã‚‹ä¸€ã¤ã®é¦–å°¾ä¸€è²«ã—ãŸè¦ç´„ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\nè¦ç´„ç¾¤:\n{combined}"
+            else:
+                prompt = f"Combine these summaries into one coherent summary for query: '{state.current_query}'.\n\nSummaries:\n{combined}"
+            state.new_information = await self.llm_client.generate_text(prompt=prompt)
+            state.accumulated_summary += f"\n\n## {state.current_query}\n{state.new_information}"
+            if progress_callback: await progress_callback("Summary update complete.")
+
+        for res in selected_results:
+            if res['link'] not in [s['link'] for s in state.sources_gathered]:
+                state.sources_gathered.append(Source(title=res['title'], link=res['link']))
+
+        state.pending_source_selection = False
+
+    async def extract_entities_and_relations(self, state: ResearchState, progress_callback: Optional[Callable[[str], None]] = None):
+        if not state.new_information or len(state.new_information) < 20: return
+
+        logger.info("Extracting entities and relations (structured).")
+        if progress_callback: await progress_callback("Extracting entities and relations for knowledge graph...")
+        if state.language == "Japanese":
+            prompt = f"ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸»è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚ã‚’ç‰¹å®šã—ã¦ãã ã•ã„:\n\n{state.new_information}"
+        else:
+            prompt = f"Identify key entities and relationships from this text:\n\n{state.new_information}"
+
+        try:
+            kg_model = await self.llm_client.generate_structured(prompt=prompt, response_model=KnowledgeGraphModel)
+
+            # Merge nodes
+            existing_node_ids = {n['id'] for n in state.knowledge_graph_nodes}
+            for n in kg_model.nodes:
+                if n.id not in existing_node_ids:
+                    state.knowledge_graph_nodes.append(n.model_dump())
+                    existing_node_ids.add(n.id)
+
+            # Merge edges (simplified deduplication based on source/target/label)
+            existing_edge_keys = {(e['source'], e['target'], e.get('label')) for e in state.knowledge_graph_edges}
+            for e in kg_model.edges:
+                edge_key = (e.source, e.target, e.label)
+                if edge_key not in existing_edge_keys:
+                    state.knowledge_graph_edges.append(e.model_dump())
+                    existing_edge_keys.add(edge_key)
+
+            if progress_callback: await progress_callback(f"Knowledge graph now has {len(state.knowledge_graph_nodes)} nodes and {len(state.knowledge_graph_edges)} edges.")
+        except Exception as e:
+            logger.error(f"KG extraction failed: {e}")
+            if progress_callback: await progress_callback("Knowledge graph extraction skipped or failed.")
diff --git a/deep_research_project/core/planning.py b/deep_research_project/core/planning.py
new file mode 100644
index 0000000..8226198
--- /dev/null
+++ b/deep_research_project/core/planning.py
@@ -0,0 +1,59 @@
+from typing import List, Optional, Callable
+import logging
+from deep_research_project.config.config import Configuration
+from deep_research_project.core.state import ResearchPlanModel
+from deep_research_project.tools.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+class PlanGenerator:
+    def __init__(self, llm_client: LLMClient, config: Configuration):
+        self.llm_client = llm_client
+        self.config = config
+
+    async def generate_plan(self, topic: str, language: str, progress_callback: Optional[Callable[[str], None]] = None) -> List[dict]:
+        logger.info(f"Generating research plan for topic: {topic} (Language: {language})")
+        if progress_callback: await progress_callback("Generating structured research plan...")
+
+        try:
+            min_sec = getattr(self.config, "RESEARCH_PLAN_MIN_SECTIONS", 3)
+            max_sec = getattr(self.config, "RESEARCH_PLAN_MAX_SECTIONS", 5)
+            if language == "Japanese":
+                prompt = (
+                    f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦ã€{min_sec}ã€œ{max_sec}ã¤ã®ä¸»è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã‚‹æ§‹é€ åŒ–ã•ã‚ŒãŸãƒªã‚µãƒ¼ãƒè¨ˆç”»ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
+                    f"ãƒªã‚µãƒ¼ãƒãƒˆãƒ”ãƒƒã‚¯: {topic}\n\n"
+                    f"å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªã‚µãƒ¼ãƒã™ã¹ãå†…å®¹ã®ç°¡æ½”ãªèª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
+                )
+            else:
+                prompt = (
+                    f"Based on the following research topic, generate a structured research plan consisting of {min_sec} to {max_sec} key sections.\n"
+                    f"Research Topic: {topic}\n\n"
+                    f"For each section, provide a title and a brief description of what should be researched."
+                )
+
+            plan_model = await self.llm_client.generate_structured(prompt=prompt, response_model=ResearchPlanModel)
+
+            research_plan = []
+            for sec in plan_model.sections:
+                research_plan.append({
+                    "title": sec.title,
+                    "description": sec.description,
+                    "status": "pending",
+                    "summary": "",
+                    "sources": []
+                })
+
+            # Visibility Enhancement: Emit plan details as formatted message
+            plan_str = "## ğŸ“‹ Research Plan\n\n"
+            for i, sec in enumerate(research_plan):
+                plan_str += f"{i+1}. **{sec['title']}**\n   - {sec['description']}\n\n"
+            if progress_callback:
+                await progress_callback(plan_str)
+
+            logger.info(f"Research plan generated with {len(research_plan)} sections.")
+            return research_plan
+
+        except Exception as e:
+            logger.error(f"Error generating research plan: {e}", exc_info=True)
+            # Return a default plan in case of failure
+            return [{"title": "General Research", "description": f"Research on {topic}", "status": "pending", "summary": "", "sources": []}]
diff --git a/deep_research_project/core/reflection.py b/deep_research_project/core/reflection.py
new file mode 100644
index 0000000..4008626
--- /dev/null
+++ b/deep_research_project/core/reflection.py
@@ -0,0 +1,43 @@
+from typing import Optional, Callable, Tuple
+import logging
+from deep_research_project.config.config import Configuration
+from deep_research_project.tools.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+class ReflectionManager:
+    def __init__(self, llm_client: LLMClient, config: Configuration):
+        self.llm_client = llm_client
+        self.config = config
+
+    async def reflect_on_summary(self, topic: str, section_title: str, accumulated_summary: str, language: str, progress_callback: Optional[Callable[[str], None]] = None) -> Tuple[Optional[str], str]:
+        if progress_callback: await progress_callback(f"Reflecting on findings for section: '{section_title}'...")
+        if language == "Japanese":
+            prompt = (
+                f"ãƒˆãƒ”ãƒƒã‚¯: {topic}\n"
+                f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section_title}\n"
+                f"ç¾åœ¨ã®è¦ç´„:\n{accumulated_summary}\n\n"
+                f"ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã•ã‚‰ãªã‚‹èª¿æŸ»ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚"
+                f"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: EVALUATION: <CONTINUE|CONCLUDE>\nQUERY: <æ¬¡ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¾ãŸã¯ None>"
+            )
+        else:
+            prompt = (
+                f"Topic: {topic}\n"
+                f"Section: {section_title}\n"
+                f"Current Summary:\n{accumulated_summary}\n\n"
+                f"Evaluate if more research is needed for this section. "
+                f"Format: EVALUATION: <CONTINUE|CONCLUDE>\nQUERY: <Next search query or None>"
+            )
+
+        response = await self.llm_client.generate_text(prompt=prompt)
+        # Simple parsing for reflection
+        lines = response.split('\n')
+        evaluation = "CONCLUDE"
+        next_query = None
+        for line in lines:
+            if "EVALUATION:" in line.upper(): evaluation = line.split(":")[-1].strip().upper()
+            if "QUERY:" in line.upper():
+                q = line.split(":")[-1].strip()
+                if q.lower() != "none": next_query = q
+
+        return next_query, evaluation
diff --git a/deep_research_project/core/reporting.py b/deep_research_project/core/reporting.py
new file mode 100644
index 0000000..ae96c3a
--- /dev/null
+++ b/deep_research_project/core/reporting.py
@@ -0,0 +1,87 @@
+from typing import List, Dict, Optional, Callable
+import logging
+from deep_research_project.config.config import Configuration
+from deep_research_project.tools.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+class ReportGenerator:
+    def __init__(self, llm_client: LLMClient, config: Configuration):
+        self.llm_client = llm_client
+        self.config = config
+
+    async def finalize_report(self, topic: str, research_plan: List[Dict], language: str, progress_callback: Optional[Callable[[str], None]] = None) -> str:
+        logger.info("Finalizing report with citations.")
+
+        full_context = ""
+        all_sources = []
+        if research_plan:
+            for i, sec in enumerate(research_plan):
+                if sec['summary']:
+                    full_context += f"\n\n### {sec['title']}\n{sec['summary']}"
+                for s in sec['sources']:
+                    if s['link'] not in [src['link'] for src in all_sources]:
+                        all_sources.append(s)
+
+        source_list_str = "\n".join([f"[{i+1}] {s['title']} ({s['link']})" for i, s in enumerate(all_sources)])
+
+        if not source_list_str:
+            source_info = "No specific web sources were found or selected for this research."
+            citation_instruction = "Since no sources are available, do not use in-text citations."
+        else:
+            source_info = f"Reference Sources:\n{source_list_str}"
+            if language == "Japanese":
+                citation_instruction = "ä¸Šè¨˜ã®ã‚½ãƒ¼ã‚¹ã«æƒ…å ±ã‚’å¸°å±ã•ã›ã‚‹ãŸã‚ã«ã€[1]ã‚„[2, 3]ã®ã‚ˆã†ãªç•ªå·ä»˜ãã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å¼•ç”¨ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
+            else:
+                citation_instruction = "You MUST use numbered in-text citations such as [1] or [2, 3] to attribute information to the sources listed above."
+
+        if language == "Japanese":
+            prompt = (
+                f"ãƒˆãƒ”ãƒƒã‚¯: {topic} ã«é–¢ã™ã‚‹æœ€çµ‚çš„ãªãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆã—ã¦ãã ã•ã„ã€‚\n\n"
+                f"ãƒªã‚µãƒ¼ãƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®è¦ç´„ï¼‰:\n{full_context}\n\n"
+                f"{source_info}\n\n"
+                f"å³æ ¼ãªæŒ‡ç¤º:\n"
+                f"1. ãƒ¬ãƒãƒ¼ãƒˆã¯åŒ…æ‹¬çš„ã§ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã‚ã‚Šã€æ˜ç¢ºãªè¦‹å‡ºã—ã‚’ä¼´ã†æ§‹é€ ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
+                f"2. {citation_instruction}\n"
+                f"3. ã‚½ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã€ã™ã¹ã¦ã®ä¸»è¦ãªä¸»å¼µã‚„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«ã¯å¼•ç”¨ã‚’ä»˜ã‘ã‚‹ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚\n"
+                f"4. æä¾›ã•ã‚ŒãŸãƒªã‚¹ãƒˆã«ãªã„ã‚½ãƒ¼ã‚¹ã«ã¯è¨€åŠã—ãªã„ã§ãã ã•ã„ã€‚\n"
+                f"5. æœ€å¾Œã«èª¿æŸ»çµæœã®ã¾ã¨ã‚ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
+            )
+        else:
+            prompt = (
+                f"Synthesize a final research report for the topic: {topic}\n\n"
+                f"Research Context (Summaries from various sections):\n{full_context}\n\n"
+                f"{source_info}\n\n"
+                f"STRICT INSTRUCTIONS:\n"
+                f"1. The report must be comprehensive, professional, and well-structured with clear headings.\n"
+                f"2. {citation_instruction}\n"
+                f"3. Every major claim or data point should ideally be cited if sources are available.\n"
+                f"4. Do not mention sources that are not in the provided list.\n"
+                f"5. End with a summary of the findings."
+            )
+
+        if progress_callback: await progress_callback("Synthesizing final research report with all findings...")
+        report = await self.llm_client.generate_text(prompt=prompt)
+
+        sources_section = f"\n\n## Sources\n{source_list_str}" if source_list_str else ""
+        final_report = f"{report}{sources_section}"
+        if progress_callback: await progress_callback("Final report generation complete.")
+
+        return final_report
+
+    def format_follow_up_prompt(self, final_report: str, question: str, language: str) -> str:
+        """Formats the prompt for a follow-up question based on the final report."""
+        if language == "Japanese":
+            return (
+                f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\n"
+                f"ãƒ¬ãƒãƒ¼ãƒˆ:\n{final_report}\n\n"
+                f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}\n\n"
+                f"ãƒ¬ãƒãƒ¼ãƒˆã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦ã€æ˜ç¢ºã§ç°¡æ½”ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚"
+            )
+        else:
+            return (
+                f"Based on the following research report, answer the user's follow-up question.\n\n"
+                f"Report:\n{final_report}\n\n"
+                f"User Question: {question}\n\n"
+                f"Provide a clear and concise answer based only on the report content."
+            )
diff --git a/deep_research_project/core/research_loop.py b/deep_research_project/core/research_loop.py
index 1134cdd..d076d01 100644
--- a/deep_research_project/core/research_loop.py
+++ b/deep_research_project/core/research_loop.py
@@ -7,6 +7,11 @@
 from deep_research_project.tools.llm_client import LLMClient
 from deep_research_project.tools.search_client import SearchClient
 from deep_research_project.tools.content_retriever import ContentRetriever
+from deep_research_project.core.planning import PlanGenerator
+from deep_research_project.core.execution import ExecutionManager
+from deep_research_project.core.reflection import ReflectionManager
+from deep_research_project.core.reporting import ReportGenerator
+
 import logging
 import json
 import asyncio
@@ -15,26 +20,6 @@
 logger = logging.getLogger(__name__)
 
 
-def split_text_into_chunks(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
-    """Splits a given text into overlapping chunks."""
-    if not text: return []
-    if chunk_size <= 0: raise ValueError("Chunk size must be positive.")
-    if chunk_overlap < 0 or chunk_overlap >= chunk_size:
-        raise ValueError("Invalid chunk overlap.")
-
-    text_len = len(text)
-    if text_len <= chunk_size: return [text]
-
-    chunks = []
-    idx = 0
-    while idx < text_len:
-        end_idx = idx + chunk_size
-        chunks.append(text[idx:end_idx])
-        if end_idx >= text_len: break
-        idx += (chunk_size - chunk_overlap)
-    return chunks
-
-
 class ResearchLoop:
     def __init__(self, config: Configuration, state: ResearchState, progress_callback: Optional[Callable[[str], None]] = None):
         self.config = config
@@ -46,52 +31,18 @@ def __init__(self, config: Configuration, state: ResearchState, progress_callbac
         self.search_client = SearchClient(config)
         self.content_retriever = ContentRetriever(config=self.config, progress_callback=progress_callback)
 
+        self.planner = PlanGenerator(self.llm_client, config)
+        self.executor = ExecutionManager(self.llm_client, self.search_client, self.content_retriever, config)
+        self.reflector = ReflectionManager(self.llm_client, config)
+        self.reporter = ReportGenerator(self.llm_client, config)
+
     async def _generate_research_plan(self):
-        logger.info(f"Generating research plan for topic: {self.state.research_topic} (Language: {self.state.language})")
-        if self.progress_callback: await self.progress_callback("Generating structured research plan...")
-
-        try:
-            min_sec = getattr(self.config, "RESEARCH_PLAN_MIN_SECTIONS", 3)
-            max_sec = getattr(self.config, "RESEARCH_PLAN_MAX_SECTIONS", 5)
-            if self.state.language == "Japanese":
-                prompt = (
-                    f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦ã€{min_sec}ã€œ{max_sec}ã¤ã®ä¸»è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã‚‹æ§‹é€ åŒ–ã•ã‚ŒãŸãƒªã‚µãƒ¼ãƒè¨ˆç”»ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
-                    f"ãƒªã‚µãƒ¼ãƒãƒˆãƒ”ãƒƒã‚¯: {self.state.research_topic}\n\n"
-                    f"å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªã‚µãƒ¼ãƒã™ã¹ãå†…å®¹ã®ç°¡æ½”ãªèª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
-                )
-            else:
-                prompt = (
-                    f"Based on the following research topic, generate a structured research plan consisting of {min_sec} to {max_sec} key sections.\n"
-                    f"Research Topic: {self.state.research_topic}\n\n"
-                    f"For each section, provide a title and a brief description of what should be researched."
-                )
-
-            plan_model = await self.llm_client.generate_structured(prompt=prompt, response_model=ResearchPlanModel)
-
-            self.state.research_plan = []
-            for sec in plan_model.sections:
-                self.state.research_plan.append({
-                    "title": sec.title,
-                    "description": sec.description,
-                    "status": "pending",
-                    "summary": "",
-                    "sources": []
-                })
-
-            self.state.current_section_index = -1
-            
-            # Visibility Enhancement: Emit plan details as formatted message
-            plan_str = "## ğŸ“‹ Research Plan\n\n"
-            for i, sec in enumerate(self.state.research_plan):
-                plan_str += f"{i+1}. **{sec['title']}**\n   - {sec['description']}\n\n"
-            if self.progress_callback: 
-                await self.progress_callback(plan_str)
-            
-            logger.info(f"Research plan generated with {len(self.state.research_plan)} sections.")
-        except Exception as e:
-            logger.error(f"Error generating research plan: {e}", exc_info=True)
-            self.state.research_plan = [{"title": "General Research", "description": f"Research on {self.state.research_topic}", "status": "pending", "summary": "", "sources": []}]
-            self.state.current_section_index = -1
+        self.state.research_plan = await self.planner.generate_plan(
+            self.state.research_topic,
+            self.state.language,
+            self.progress_callback
+        )
+        self.state.current_section_index = -1
 
     def _get_current_section(self):
         if 0 <= self.state.current_section_index < len(self.state.research_plan):
@@ -103,189 +54,33 @@ async def _generate_initial_query(self):
         topic = section['title'] if section else self.state.research_topic
         desc = section['description'] if section else ""
 
-        logger.info(f"Generating initial query for: {topic}")
-        try:
-            max_words = getattr(self.config, "MAX_QUERY_WORDS", 12)
-            if self.state.language == "Japanese":
-                prompt = (
-                    f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€ç°¡æ½”ãªWebæ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆæœ€å¤§{max_words}å˜èªï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
-                    f"ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ: {self.state.research_topic}\n"
-                    f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {topic}\n"
-                    f"èª¬æ˜: {desc}\n\n"
-                    f"ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è‹±èªã®ã‚½ãƒ¼ã‚¹ã‚‚å–å¾—ã§ãã‚‹ã‚ˆã†ã€é©åˆ‡ã§ã‚ã‚Œã°è‹±èªã®ã‚¯ã‚¨ãƒªã‚‚æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
-                )
-            else:
-                prompt = (
-                    f"Generate a concise web search query (max {max_words} words) for the following research task.\n"
-                    f"Main Topic: {self.state.research_topic}\n"
-                    f"Section: {topic}\n"
-                    f"Description: {desc}\n\n"
-                    f"Output only the query."
-                )
-            query = await self.llm_client.generate_text(prompt=prompt)
-            self.state.proposed_query = query
-            self.state.current_query = None
-        except Exception as e:
-            logger.error(f"Error generating query: {e}")
-            self.state.proposed_query = None
+        query = await self.executor.generate_initial_query(self.state, topic, desc)
+        self.state.proposed_query = query
+        self.state.current_query = None
 
     async def _web_search(self):
         if not self.state.current_query: return
-        logger.info(f"Performing web search for: {self.state.current_query}")
-        if self.progress_callback: await self.progress_callback(f"Searching web for: '{self.state.current_query}'...")
-        try:
-            results = await self.search_client.search(self.state.current_query, num_results=self.config.MAX_SEARCH_RESULTS_PER_QUERY)
-            self.state.search_results = results
-            self.state.pending_source_selection = bool(results)
-            if self.progress_callback:
-                if results:
-                    results_str = "\n".join([f"- [{r['title']}]({r['link']})" for r in results])
-                    await self.progress_callback(f"Found {len(results)} potential sources:\n{results_str}")
-                else:
-                    await self.progress_callback("No search results found.")
-        except Exception as e:
-            logger.error(f"Error during search: {e}")
-            self.state.search_results = []
-            self.state.pending_source_selection = False
-            if self.progress_callback: await self.progress_callback(f"Search failed: {e}")
+        results = await self.executor.web_search(self.state.current_query, self.progress_callback)
+        self.state.search_results = results
+        self.state.pending_source_selection = bool(results)
 
     async def _summarize_sources(self, selected_results: List[SearchResult]):
-        if not selected_results:
-            self.state.new_information = "No sources selected."
-            self.state.pending_source_selection = False
-            return
-
-        if self.progress_callback:
-             sources_titles = ", ".join([r['title'] for r in selected_results])
-             await self.progress_callback(f"Summarizing {len(selected_results)} sources: {sources_titles}...")
-
-        all_chunk_summaries = []
-        all_chunks_info = [] # Store all chunks to be processed in parallel
-        if self.state.fetched_content is None: self.state.fetched_content = {}
-
-        for result in selected_results:
-            url = result['link']
-            if url not in self.state.fetched_content:
-                if self.config.USE_SNIPPETS_ONLY_MODE:
-                    content = result.get('snippet', '')
-                else:
-                    content = await self.content_retriever.retrieve_and_extract(url)
-                    if not content: content = result.get('snippet', '')
-                self.state.fetched_content[url] = content
-
-            content = self.state.fetched_content[url]
-            chunks = split_text_into_chunks(content, self.config.SUMMARIZATION_CHUNK_SIZE_CHARS, self.config.SUMMARIZATION_CHUNK_OVERLAP_CHARS)
-            all_chunks_info.extend([(chunk, url) for chunk in chunks])
-
-        # Limit concurrency using Semaphore
-        semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_CHUNKS)
-
-        async def summarize_chunk(chunk_info):
-            chunk, url = chunk_info
-            if self.state.is_interrupted: return None
-            async with semaphore:
-                if self.progress_callback: await self.progress_callback(f"Summarizing chunk from {url}...")
-                if self.state.language == "Japanese":
-                    prompt = f"ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ãƒª: '{self.state.current_query}' ã®ãŸã‚ã«ã€ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã‚»ã‚°ãƒ¡ãƒ³ãƒˆ:\n{chunk}"
-                else:
-                    prompt = f"Summarize this segment for the research query: '{self.state.current_query}'.\n\nSegment:\n{chunk}"
-                return await self.llm_client.generate_text(prompt=prompt)
-
-        # Execute parallel summarization
-        if all_chunks_info:
-            if self.progress_callback: await self.progress_callback(f"Starting parallel summarization for {len(all_chunks_info)} chunks...")
-            summaries = await asyncio.gather(*[summarize_chunk(info) for info in all_chunks_info])
-            all_chunk_summaries.extend([s for s in summaries if s])
-        
-        if self.state.is_interrupted:
-             return
-
-        if not all_chunk_summaries:
-            self.state.new_information = "Could not summarize any content."
-            if self.progress_callback: await self.progress_callback("No content could be summarized.")
-        else:
-            if self.progress_callback: await self.progress_callback("Synthesizing final summary for the query...")
-            combined = "\n\n---\n\n".join(all_chunk_summaries)
-            if self.state.language == "Japanese":
-                prompt = f"ã“ã‚Œã‚‰ã®è¦ç´„ã‚’ã€ã‚¯ã‚¨ãƒª: '{self.state.current_query}' ã«é–¢ã™ã‚‹ä¸€ã¤ã®é¦–å°¾ä¸€è²«ã—ãŸè¦ç´„ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\nè¦ç´„ç¾¤:\n{combined}"
-            else:
-                prompt = f"Combine these summaries into one coherent summary for query: '{self.state.current_query}'.\n\nSummaries:\n{combined}"
-            self.state.new_information = await self.llm_client.generate_text(prompt=prompt)
-            self.state.accumulated_summary += f"\n\n## {self.state.current_query}\n{self.state.new_information}"
-            if self.progress_callback: await self.progress_callback("Summary update complete.")
-
-        for res in selected_results:
-            if res['link'] not in [s['link'] for s in self.state.sources_gathered]:
-                self.state.sources_gathered.append(Source(title=res['title'], link=res['link']))
-
-        self.state.pending_source_selection = False
-        # await self._extract_entities_and_relations() # Now handled in parallel with reflection
+        await self.executor.summarize_sources(self.state, selected_results, self.progress_callback)
 
     async def _extract_entities_and_relations(self):
-        if not self.state.new_information or len(self.state.new_information) < 20: return
-
-        logger.info("Extracting entities and relations (structured).")
-        if self.progress_callback: await self.progress_callback("Extracting entities and relations for knowledge graph...")
-        if self.state.language == "Japanese":
-            prompt = f"ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸»è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚ã‚’ç‰¹å®šã—ã¦ãã ã•ã„:\n\n{self.state.new_information}"
-        else:
-            prompt = f"Identify key entities and relationships from this text:\n\n{self.state.new_information}"
-
-        try:
-            kg_model = await self.llm_client.generate_structured(prompt=prompt, response_model=KnowledgeGraphModel)
-
-            # Merge nodes
-            existing_node_ids = {n['id'] for n in self.state.knowledge_graph_nodes}
-            for n in kg_model.nodes:
-                if n.id not in existing_node_ids:
-                    self.state.knowledge_graph_nodes.append(n.model_dump())
-                    existing_node_ids.add(n.id)
-
-            # Merge edges (simplified deduplication based on source/target/label)
-            existing_edge_keys = {(e['source'], e['target'], e.get('label')) for e in self.state.knowledge_graph_edges}
-            for e in kg_model.edges:
-                edge_key = (e.source, e.target, e.label)
-                if edge_key not in existing_edge_keys:
-                    self.state.knowledge_graph_edges.append(e.model_dump())
-                    existing_edge_keys.add(edge_key)
-
-            if self.progress_callback: await self.progress_callback(f"Knowledge graph now has {len(self.state.knowledge_graph_nodes)} nodes and {len(self.state.knowledge_graph_edges)} edges.")
-        except Exception as e:
-            logger.error(f"KG extraction failed: {e}")
-            if self.progress_callback: await self.progress_callback("Knowledge graph extraction skipped or failed.")
+        await self.executor.extract_entities_and_relations(self.state, self.progress_callback)
 
     async def _reflect_on_summary(self):
         section = self._get_current_section()
         title = section['title'] if section else "General"
 
-        if self.progress_callback: await self.progress_callback(f"Reflecting on findings for section: '{title}'...")
-        if self.state.language == "Japanese":
-            prompt = (
-                f"ãƒˆãƒ”ãƒƒã‚¯: {self.state.research_topic}\n"
-                f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {title}\n"
-                f"ç¾åœ¨ã®è¦ç´„:\n{self.state.accumulated_summary}\n\n"
-                f"ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã•ã‚‰ãªã‚‹èª¿æŸ»ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚"
-                f"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: EVALUATION: <CONTINUE|CONCLUDE>\nQUERY: <æ¬¡ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¾ãŸã¯ None>"
-            )
-        else:
-            prompt = (
-                f"Topic: {self.state.research_topic}\n"
-                f"Section: {title}\n"
-                f"Current Summary:\n{self.state.accumulated_summary}\n\n"
-                f"Evaluate if more research is needed for this section. "
-                f"Format: EVALUATION: <CONTINUE|CONCLUDE>\nQUERY: <Next search query or None>"
-            )
-
-        response = await self.llm_client.generate_text(prompt=prompt)
-        # Simple parsing for reflection
-        lines = response.split('\n')
-        evaluation = "CONCLUDE"
-        next_query = None
-        for line in lines:
-            if "EVALUATION:" in line.upper(): evaluation = line.split(":")[-1].strip().upper()
-            if "QUERY:" in line.upper():
-                q = line.split(":")[-1].strip()
-                if q.lower() != "none": next_query = q
+        next_query, evaluation = await self.reflector.reflect_on_summary(
+            self.state.research_topic,
+            title,
+            self.state.accumulated_summary,
+            self.state.language,
+            self.progress_callback
+        )
 
         if "CONCLUDE" in evaluation:
             self.state.proposed_query = None
@@ -293,78 +88,15 @@ async def _reflect_on_summary(self):
             self.state.proposed_query = next_query
 
     async def _finalize_summary(self):
-        logger.info("Finalizing report with citations.")
-
-        full_context = ""
-        all_sources = []
-        if self.state.research_plan:
-            for i, sec in enumerate(self.state.research_plan):
-                if sec['summary']:
-                    full_context += f"\n\n### {sec['title']}\n{sec['summary']}"
-                for s in sec['sources']:
-                    if s['link'] not in [src['link'] for src in all_sources]:
-                        all_sources.append(s)
-
-        source_list_str = "\n".join([f"[{i+1}] {s['title']} ({s['link']})" for i, s in enumerate(all_sources)])
-
-        if not source_list_str:
-            source_info = "No specific web sources were found or selected for this research."
-            citation_instruction = "Since no sources are available, do not use in-text citations."
-        else:
-            source_info = f"Reference Sources:\n{source_list_str}"
-            if self.state.language == "Japanese":
-                citation_instruction = "ä¸Šè¨˜ã®ã‚½ãƒ¼ã‚¹ã«æƒ…å ±ã‚’å¸°å±ã•ã›ã‚‹ãŸã‚ã«ã€[1]ã‚„[2, 3]ã®ã‚ˆã†ãªç•ªå·ä»˜ãã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å¼•ç”¨ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
-            else:
-                citation_instruction = "You MUST use numbered in-text citations such as [1] or [2, 3] to attribute information to the sources listed above."
-
-        if self.state.language == "Japanese":
-            prompt = (
-                f"ãƒˆãƒ”ãƒƒã‚¯: {self.state.research_topic} ã«é–¢ã™ã‚‹æœ€çµ‚çš„ãªãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆã—ã¦ãã ã•ã„ã€‚\n\n"
-                f"ãƒªã‚µãƒ¼ãƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®è¦ç´„ï¼‰:\n{full_context}\n\n"
-                f"{source_info}\n\n"
-                f"å³æ ¼ãªæŒ‡ç¤º:\n"
-                f"1. ãƒ¬ãƒãƒ¼ãƒˆã¯åŒ…æ‹¬çš„ã§ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã‚ã‚Šã€æ˜ç¢ºãªè¦‹å‡ºã—ã‚’ä¼´ã†æ§‹é€ ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
-                f"2. {citation_instruction}\n"
-                f"3. ã‚½ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã€ã™ã¹ã¦ã®ä¸»è¦ãªä¸»å¼µã‚„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«ã¯å¼•ç”¨ã‚’ä»˜ã‘ã‚‹ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚\n"
-                f"4. æä¾›ã•ã‚ŒãŸãƒªã‚¹ãƒˆã«ãªã„ã‚½ãƒ¼ã‚¹ã«ã¯è¨€åŠã—ãªã„ã§ãã ã•ã„ã€‚\n"
-                f"5. æœ€å¾Œã«èª¿æŸ»çµæœã®ã¾ã¨ã‚ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
-            )
-        else:
-            prompt = (
-                f"Synthesize a final research report for the topic: {self.state.research_topic}\n\n"
-                f"Research Context (Summaries from various sections):\n{full_context}\n\n"
-                f"{source_info}\n\n"
-                f"STRICT INSTRUCTIONS:\n"
-                f"1. The report must be comprehensive, professional, and well-structured with clear headings.\n"
-                f"2. {citation_instruction}\n"
-                f"3. Every major claim or data point should ideally be cited if sources are available.\n"
-                f"4. Do not mention sources that are not in the provided list.\n"
-                f"5. End with a summary of the findings."
-            )
-
-        if self.progress_callback: await self.progress_callback("Synthesizing final research report with all findings...")
-        report = await self.llm_client.generate_text(prompt=prompt)
-
-        sources_section = f"\n\n## Sources\n{source_list_str}" if source_list_str else ""
-        self.state.final_report = f"{report}{sources_section}"
-        if self.progress_callback: await self.progress_callback("Final report generation complete.")
+        self.state.final_report = await self.reporter.finalize_report(
+            self.state.research_topic,
+            self.state.research_plan,
+            self.state.language,
+            self.progress_callback
+        )
 
     def format_follow_up_prompt(self, final_report: str, question: str) -> str:
-        """Formats the prompt for a follow-up question based on the final report."""
-        if self.state.language == "Japanese":
-            return (
-                f"ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\n"
-                f"ãƒ¬ãƒãƒ¼ãƒˆ:\n{final_report}\n\n"
-                f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}\n\n"
-                f"ãƒ¬ãƒãƒ¼ãƒˆã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦ã€æ˜ç¢ºã§ç°¡æ½”ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚"
-            )
-        else:
-            return (
-                f"Based on the following research report, answer the user's follow-up question.\n\n"
-                f"Report:\n{final_report}\n\n"
-                f"User Question: {question}\n\n"
-                f"Provide a clear and concise answer based only on the report content."
-            )
+        return self.reporter.format_follow_up_prompt(final_report, question, self.state.language)
 
     async def _process_section(self, section):
         """Processes a single research section."""
diff --git a/deep_research_project/core/utils.py b/deep_research_project/core/utils.py
new file mode 100644
index 0000000..f7c8f37
--- /dev/null
+++ b/deep_research_project/core/utils.py
@@ -0,0 +1,20 @@
+from typing import List
+
+def split_text_into_chunks(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
+    """Splits a given text into overlapping chunks."""
+    if not text: return []
+    if chunk_size <= 0: raise ValueError("Chunk size must be positive.")
+    if chunk_overlap < 0 or chunk_overlap >= chunk_size:
+        raise ValueError("Invalid chunk overlap.")
+
+    text_len = len(text)
+    if text_len <= chunk_size: return [text]
+
+    chunks = []
+    idx = 0
+    while idx < text_len:
+        end_idx = idx + chunk_size
+        chunks.append(text[idx:end_idx])
+        if end_idx >= text_len: break
+        idx += (chunk_size - chunk_overlap)
+    return chunks
diff --git a/deep_research_project/tests/test_core_components.py b/deep_research_project/tests/test_core_components.py
index 9e0c8de..18a4e80 100644
--- a/deep_research_project/tests/test_core_components.py
+++ b/deep_research_project/tests/test_core_components.py
@@ -5,7 +5,8 @@
 
 # Modules to be tested
 from deep_research_project.config.config import Configuration
-from deep_research_project.core.research_loop import ResearchLoop, split_text_into_chunks
+from deep_research_project.core.research_loop import ResearchLoop
+from deep_research_project.core.utils import split_text_into_chunks
 from deep_research_project.core.state import ResearchState, Source, SearchResult, ResearchPlanModel, Section, KnowledgeGraphModel
 from deep_research_project.tools.llm_client import LLMClient
 
