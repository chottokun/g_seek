# Deep Research Project

## 実行方法（必ずプロジェクトルートで実行してください）

```sh
python -m deep_research_project.main
```
または（uvを使う場合）
```sh
uv run -m deep_research_project.main
```

- `.env` ファイルはプロジェクトルート（g_seek）に置いてください。
- 直接 `deep_research_project` ディレクトリで `python main.py` などと実行しないでください。
- import文はすべて絶対import（`from deep_research_project...`）で統一されています。

# Architecture

The two LangChain projects share a multi-stage “research assistant” architecture, but differ in orchestration style.  **Open Deep Research** supports two modes: a **plan-and-execute graph workflow** (sequential) and a **multi-agent** (parallel) architecture. In the graph mode, a planner LLM first creates a structured report outline (sections), which the system then executes section-by-section. Each section loop involves generating search queries, retrieving documents, summarizing them, and reflecting on gaps. The multi-agent mode assigns a **Supervisor** agent to plan and combine outputs, and multiple **Researcher** agents to work in parallel on different sections. Each Researcher agent has dedicated tools (e.g. search) for its section, while the Supervisor aggregates results.

In contrast, the **Local Deep Researcher** uses a single-agent **iterative loop** inspired by the IterDRAG approach. It decomposes a user topic into a search query, retrieves web results, uses an LLM to summarize them, then has the LLM *reflect* on the summary to identify knowledge gaps and generate a new query. This loop repeats for a configured number of iterations, each time appending new information to the growing summary.  Overall, the architectures can be summarized as:

* **OpenDeepResearch (Graph-based)**: A LangGraph-driven DAG where state (topic, plan, section queries, summaries) flows through nodes. Key phases: *plan creation*, *plan approval*, then repeated *search → summarize → (feedback)* per section. Multiple search tools are supported (Tavily, Perplexity, Exa, etc.).
* **OpenDeepResearch (Multi-Agent)**: A LangGraph-driven multi-agent DAG with a **Supervisor** (plans sections, assembles report) and multiple **Researcher** agents (each researches/writes one section in parallel). Each researcher uses tools to fetch content; currently only Tavily search is supported.
* **LocalDeepResearcher**: A LangGraph DAG (or chain-of-tasks) forming an iterative loop. State includes the current search query, gathered sources, and the accumulating summary. Workflow per loop:

  1. LLM generates an initial web search query for the topic.
  2. Search tool retrieves results.
  3. LLM summarizes the search results.
  4. LLM reflects on the summary to find gaps and produces the next query.
  5. Repeat until iteration limit.
     Final output is a markdown report with sources.

These architectures emphasize **RAG-style retrieval**: external web results are fed into the LLM for generation (summaries, reflections), and **agent orchestration**: using LangGraph to sequence or parallelize LLM-driven tasks.

# Core Logic

Both repos automate multi-step research by interleaving search and generation:

* **Planning/Outline:** (open\_deep\_research only) The assistant first creates a report plan. An LLM planner analyzes the topic and breaks it into sections or sub-questions. In the graph mode, this plan is shown to the user for feedback (accept or refine) before proceeding.

* **Search Query Generation:** For each section or iteration, an LLM generates one or more web search queries tailored to the sub-topic. In Local Deep Researcher, the LLM uses the *topic* (and later the current summary) to form a query. In Open Deep Research, per-section queries may be generated by a chain or agent prompt.

* **Document Retrieval:** The system uses web search tools to retrieve relevant pages. **LocalDeepResearcher** defaults to DuckDuckGo (no API key needed) but can be configured to use SearXNG, Tavily, or Perplexity. **OpenDeepResearch (Graph)** supports multiple APIs (Tavily, Perplexity, Exa, ArXiv, PubMed, LinkUp). **OpenDeepResearch (Multi-agent)** currently uses only the Tavily API for its researcher agents.

* **Summary Generation:** Retrieved documents are passed to an LLM to **summarize relevant information**. In graph-based OpenDeepResearch, this happens sequentially for each section, often with “reflection” steps between search iterations. In LocalDeepResearcher, each loop uses the LLM to produce an updated summary of all findings so far.

* **Reflection and Iteration:** After summarizing, the assistant (via the LLM) examines the summary to find what was missed. It then generates a follow-up query to dig deeper. The Local Deep Researcher explicitly “reflects on the summary to identify knowledge gaps” and creates a refined query. The graph-based OpenDeepResearch workflow similarly iterates per section (controlled by parameters like `max_search_depth`) to deepen research.

* **Aggregation:** Finally, the gathered information is combined into a single report. In the graph workflow, the final markdown report is assembled from section outputs. In multi-agent mode, the Supervisor concatenates or formats each researcher’s section into the final document. In local mode, all collected sources are formatted into a markdown summary with citations.

# Functional Components

Each implementation divides responsibilities among modules, classes, or nodes:

* **Configuration** – A central class (often named `Configuration`) holds parameters like LLM model names, search API keys, and iteration limits. For example, Local Deep Researcher’s `configuration.py` defines defaults for `LLM_PROVIDER`, `SEARCH_API`, `MAX_WEB_RESEARCH_LOOPS`, etc., which can be overridden via `.env`.

* **State/Data Classes** – LangGraph workflows use typed state objects. In Local Deep Researcher, a `SummaryState` holds fields such as `research_topic`, `search_query`, `sources_gathered`, and the growing `final_summary`. In OpenDeepResearch (graph mode) the state might similarly hold section prompts, collected texts, and final report content.

* **Graph/Workflow Definitions** –

  * *OpenDeepResearch (graph.py)*: defines a LangGraph workflow (via `StateGraph` or similar) that orchestrates planning, optional human feedback, and section loops. Key nodes/functions include “plan sections”, “generate queries”, “collect/summarize results”, and “write section content”. The README mentions functions like `init_chat_model`, parameter settings (`planner_provider/model`, `writer_provider/model`, etc.) that configure these nodes.
  * *OpenDeepResearch (multi\_agent.py)*: sets up a Supervisor agent node and multiple parallel Researcher agents (often using LangGraph’s multi-agent support). The Supervisor’s prompt and tools (e.g. “section planning”) are separate from the Researchers’ prompts (which focus on search and writing).
  * *LocalDeepResearcher (graph.py)*: defines a cycle of LangGraph nodes: e.g. `generate_query`, `web_research`, `summarize_sources`, `reflect_on_summary`, `finalize_summary` (as shown in example code). The graph includes a conditional loop: after reflecting, it either loops back to web search or ends with finalizing the summary.

* **Tool Integrations:** Each node invokes tools or LLM calls. Typical components:

  * **Search Tools:** Wrappers for DuckDuckGo, SearXNG, Tavily, Perplexity, etc. (e.g. LangChain’s `DuckDuckGoSearchAPIWrapper` or custom Tool classes).
  * **LLM Chains:** Chains or prompts for query generation, summarization, reflection, and final writing. E.g., a chain that takes search results and produces a summary paragraph.
  * **Agents (multi-agent mode):** Each agent is essentially an LLM-based chain with access to tools. The Supervisor agent has tools to plan and to collate text; Researcher agents have tools to search and to summarize/write.

* **Orchestration Logic:** Conditional edges or controller functions route the workflow. In LocalDeepResearcher, a function like `route_research(state, config)` checks if the loop count exceeds a max, deciding whether to continue searching or finalize. In OpenDeepResearch graph mode, similar logic controls section iteration. In multi-agent, LangGraph’s multi-agent runner handles parallel task scheduling.

# Web Search, RAG, and Orchestration

All variants use **retrieval-augmented generation (RAG)**: web results retrieved by search tools are fed into the LLM to ground its outputs. Each “search” step produces passages or URLs, which are aggregated (usually as text) and given to an LLM prompt for summarization or insight extraction. For example, LocalDeepResearcher’s loop uses the LLM to both **summarize search results** and then **reflect** to produce a refined query – a textbook RAG iteration. In OpenDeepResearcher, each section’s content is built by repeated search-and-summarize cycles (with human-in-the-loop for plan approval).

Search orchestration differs:

* **Sequential (Graph) Orchestration:** In the graph/workflow mode, LangGraph schedules tasks one after another. The plan is generated first, then for each section, a fixed number of search iterations run in sequence (the `max_search_depth` parameter controls how many cycles per section). The LangGraph state ensures each step’s outputs feed into the next (passing e.g. the latest summary back into the state).

* **Parallel (Multi-Agent) Orchestration:** In multi-agent mode, once the Supervisor finalizes a plan, LangGraph spawns all Researcher agents concurrently. Each agent independently performs its section’s research chain in parallel (reducing total time). This requires parallel tool usage (e.g. multiple Tavily API calls at once) and then combining outputs when all finish.

* **Iterative Loop Orchestration (Local):** The local assistant repeatedly cycles through its nodes. A conditional edge (`add_conditional_edges`) controls whether to loop again or terminate. This effectively creates a “while not done” loop in the workflow.

# Reimplementation (Streamlit + LangChain)

To re-create this logic in a Streamlit app using LangChain, one would assemble similar components:

* **LLM and Tools:** Use LangChain’s LLM wrappers (e.g. `ChatOpenAI`, `Anthropic`, or local LLM via `Ollama`/`LMStudio`) for all generation steps (planning, summarization, reflection). Use LangChain tools for search – for example, `DuckDuckGoSearchAPIWrapper`, `SearxSearchTool`, or custom `Tool` classes calling Tavily or Perplexity APIs. Environment variables (via `.env`) or config can store API keys.

* **Chains or Graphs:** Build LangChain **chains** or a **LangGraph StateGraph** to sequence the tasks.  For a graph-based approach, one could define a StateGraph much like in the repos, with nodes for query-generation, web-search, summarization, etc., and conditional edges for looping (as illustrated in the Local Deep Researcher example). For a simpler implementation, one could write a Python loop that calls LLM chains and tools in order (especially for the LocalDeepResearcher style). For multi-agent style, one could use LangChain’s agent or multithreading support to launch simultaneous section-chains and then join the results.

* **Plan and Human-in-the-Loop:** If implementing the graph workflow, present the LLM’s initial plan (section outline) via Streamlit (e.g. in text or using `st.form`). The user can accept or modify it before proceeding. This mimics the “human-in-the-loop plan approval” phase.

* **State Management:** Store intermediate results (queries, summaries, sources) in memory or session state. For example, use `st.session_state` or in-ram data structures to keep a list of URLs/sources and partial summary. Each step’s output should be appended so it can be displayed or saved. The final report can be shown as markdown with citations.

* **Output Presentation:** Use Streamlit to display progress. For example, show each search query and its top results, the summary paragraphs generated, and the evolving final markdown. You can embed the final answer in `st.markdown`. Optionally, use Streamlit’s `st.chat` or a custom UI to emulate the LangGraph Studio view of states.

* **Other Considerations:** If persistence is needed, use a vector database (e.g. Chroma) to store retrieved documents and summaries, enabling true RAG. (The example repos use direct web search without persistent store.) Also, consider LangSmith or LangChain callbacks for logging/debugging traces of the chain (though not required). Parallel sections (multi-agent) would require asynchronous functions (`asyncio` or threads) to run multiple chains concurrently and then collect outputs.

By combining these elements – an LLM, search tools, orchestrated chains, and a Streamlit UI – one can replicate the “deep research” flow: iteratively querying, retrieving, summarizing, and refining, all within a user-friendly Streamlit interface.

**Sources:** The above analysis is based on the official repos and documentation for *Open Deep Research* and *Local Deep Researcher*, which detail their architectures, workflows, and configurable components.
